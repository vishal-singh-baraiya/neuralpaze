import type { ComponentCategory } from "../types"

export const componentLibrary: ComponentCategory = {
  "Basic Layers": [
    {
      type: "Linear",
      icon: "‚ñ†",
      color: "#3B82F6",
      params: { input_size: 512, output_size: 512, bias: true },
      description: "Fully connected linear layer",
    },
    {
      type: "Conv1D",
      icon: "‚ñ¶",
      color: "#10B981",
      params: { in_channels: 64, out_channels: 64, kernel_size: 3, stride: 1, padding: 1 },
      description: "1D convolutional layer",
    },
    {
      type: "Conv2D",
      icon: "‚ñ¶",
      color: "#10B981",
      params: { in_channels: 3, out_channels: 64, kernel_size: 3, stride: 1, padding: 1 },
      description: "2D convolutional layer",
    },
    {
      type: "Embedding",
      icon: "‚äû",
      color: "#8B5CF6",
      params: { vocab_size: 50000, embed_dim: 512, padding_idx: 0 },
      description: "Token embedding layer",
    },
    {
      type: "Dropout",
      icon: "‚óä",
      color: "#F59E0B",
      params: { p: 0.1 },
      description: "Dropout regularization",
    },
    {
      type: "LayerNorm",
      icon: "‚â°",
      color: "#EF4444",
      params: { normalized_shape: 512, eps: 1e-5 },
      description: "Layer normalization",
    },
    {
      type: "BatchNorm",
      icon: "‚âà",
      color: "#EF4444",
      params: { num_features: 512, eps: 1e-5, momentum: 0.1 },
      description: "Batch normalization",
    },
  ],
  Activations: [
    { type: "ReLU", icon: "‚Üó", color: "#06B6D4", params: {}, description: "Rectified Linear Unit" },
    { type: "GELU", icon: "‚àø", color: "#06B6D4", params: {}, description: "Gaussian Error Linear Unit" },
    { type: "SiLU", icon: "‚à´", color: "#06B6D4", params: {}, description: "Sigmoid Linear Unit (Swish)" },
    { type: "Tanh", icon: "~", color: "#06B6D4", params: {}, description: "Hyperbolic tangent" },
    { type: "Softmax", icon: "‚àë", color: "#06B6D4", params: { dim: -1 }, description: "Softmax activation" },
    { type: "LeakyReLU", icon: "‚Üó", color: "#06B6D4", params: { negative_slope: 0.01 }, description: "Leaky ReLU" },
  ],
  Attention: [
    {
      type: "MultiHeadAttention",
      icon: "‚óè‚óè‚óè",
      color: "#EC4899",
      params: { d_model: 512, num_heads: 8, dropout: 0.1 },
      description: "Multi-head self-attention",
    },
    {
      type: "SparseAttention",
      icon: "‚óè‚óã‚óè",
      color: "#EC4899",
      params: { d_model: 512, num_heads: 8, sparsity_pattern: "local", window_size: 128 },
      description: "Sparse attention mechanism",
    },
    {
      type: "CrossAttention",
      icon: "‚óè√ó‚óè",
      color: "#EC4899",
      params: { d_model: 512, dropout: 0.1 },
      description: "Cross-attention layer",
    },
    {
      type: "RotaryPositionalEncoding",
      icon: "‚ü≤",
      color: "#EC4899",
      params: { d_model: 512, max_len: 8192 },
      description: "Rotary positional encoding (RoPE)",
    },
    {
      type: "ALiBi",
      icon: "üìê",
      color: "#EC4899",
      params: { num_heads: 8 },
      description: "Attention with Linear Biases",
    },
  ],
  Transformer: [
    {
      type: "TransformerBlock",
      icon: "‚äû",
      color: "#7C3AED",
      params: { d_model: 512, num_heads: 8, d_ff: 2048, dropout: 0.1 },
      description: "Standard transformer block",
    },
    {
      type: "GPTBlock",
      icon: "ü§ñ",
      color: "#7C3AED",
      params: { d_model: 768, num_heads: 12, d_ff: 3072, dropout: 0.1 },
      description: "GPT-style decoder block",
    },
    {
      type: "LlamaBlock",
      icon: "ü¶ô",
      color: "#7C3AED",
      params: { d_model: 4096, num_heads: 32, d_ff: 11008, dropout: 0.0 },
      description: "LLaMA-style transformer block",
    },
    {
      type: "FeedForward",
      icon: "‚áâ",
      color: "#7C3AED",
      params: { d_model: 512, d_ff: 2048, dropout: 0.1, activation: "gelu" },
      description: "Feed-forward network",
    },
    {
      type: "GLU",
      icon: "‚ö°",
      color: "#7C3AED",
      params: { d_model: 512, d_ff: 2048, activation: "silu" },
      description: "Gated Linear Unit",
    },
  ],
  "Advanced LLM": [
    {
      type: "MixtureOfExperts",
      icon: "üîÄ",
      color: "#F97316",
      params: { d_model: 512, num_experts: 8, top_k: 2, d_ff: 2048 },
      description: "Mixture of Experts layer",
    },
    {
      type: "GroupedQueryAttention",
      icon: "üë•",
      color: "#F97316",
      params: { d_model: 512, num_heads: 8, num_kv_heads: 2 },
      description: "Grouped Query Attention (GQA)",
    },
    {
      type: "SlidingWindowAttention",
      icon: "ü™ü",
      color: "#F97316",
      params: { d_model: 512, num_heads: 8, window_size: 4096 },
      description: "Sliding window attention",
    },
    {
      type: "RetentionLayer",
      icon: "üß†",
      color: "#F97316",
      params: { d_model: 512, num_heads: 8 },
      description: "RetNet retention mechanism",
    },
    {
      type: "MambaBlock",
      icon: "üêç",
      color: "#F97316",
      params: { d_model: 512, d_state: 16, d_conv: 4, expand: 2 },
      description: "Mamba state space model",
    },
  ],
  Custom: [
    {
      type: "CustomLayer",
      icon: "‚ö°",
      color: "#6B7280",
      params: {
        name: "MyCustomLayer",
        code: "# Write your PyTorch code here\nself.linear = nn.Linear(512, 512)\n\ndef forward(self, x):\n    return self.linear(x)",
      },
      description: "Custom PyTorch layer",
    },
    {
      type: "Residual",
      icon: "+",
      color: "#F97316",
      params: {},
      description: "Residual connection",
    },
    {
      type: "Concatenate",
      icon: "||",
      color: "#F97316",
      params: { dim: -1 },
      description: "Tensor concatenation",
    },
    {
      type: "Split",
      icon: "|/",
      color: "#F97316",
      params: { sizes: [256, 256] },
      description: "Tensor splitting",
    },
  ],
}
